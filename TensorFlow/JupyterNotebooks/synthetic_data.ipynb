{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, num_samples, input_dim, output_dim, batch_size):\n",
    "        self.num_samples = num_samples\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Generate synthetic data\n",
    "        self.inputs = np.random.rand(self.num_samples, self.input_dim).astype(\n",
    "            np.float32\n",
    "        )\n",
    "        self.targets = np.random.rand(self.num_samples, self.output_dim).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.batch_size\n",
    "        end_idx = min((idx + 1) * self.batch_size, self.num_samples)\n",
    "        batch_inputs = self.inputs[start_idx:end_idx]\n",
    "        batch_targets = self.targets[start_idx:end_idx]\n",
    "        return batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# batch_size is set here\n",
    "batch_size = 32\n",
    "\n",
    "# Base object for fitting to a sequence of data, such as a dataset.\n",
    "# Equivalent to Dataset class object in PyTorch.\n",
    "dataset = SyntheticDataset(\n",
    "    num_samples=1000, input_dim=10, output_dim=1, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Creates a `Dataset` whose elements are slices of the given tensors.\n",
    "#\n",
    "# Two tensors can be combined into one Dataset object.\n",
    "# features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
    "# labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
    "# dataset = Dataset.from_tensor_slices((features, labels))\n",
    "#\n",
    "# Both the features and the labels tensors can be converted\n",
    "# to a Dataset object separately and combined after.\n",
    "# features_dataset = Dataset.from_tensor_slices(features)\n",
    "# labels_dataset = Dataset.from_tensor_slices(labels)\n",
    "# dataset = Dataset.zip((features_dataset, labels_dataset))\n",
    "dataloader = tf.data.Dataset.from_tensor_slices((dataset.inputs, dataset.targets))\n",
    "\n",
    "# Combines consecutive elements of this dataset into batches.\n",
    "# Randomly shuffles the elements of this dataset.\n",
    "dataloader = dataloader.shuffle(buffer_size=len(dataset)).batch(batch_size)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    print(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset2(tf.data.Dataset):\n",
    "    def _generator(num_samples, input_dim, output_dim):\n",
    "        for _ in range(num_samples):\n",
    "            inputs = tf.random.normal(shape=(input_dim,))\n",
    "            targets = tf.random.normal(shape=(output_dim,))\n",
    "            yield inputs, targets\n",
    "\n",
    "    def __new__(cls, num_samples, input_dim, output_dim, **kwargs):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            lambda: cls._generator(num_samples, input_dim, output_dim),\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(input_dim,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(output_dim,), dtype=tf.float32),\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage 2:\n",
    "\n",
    "# Represents a potentially large set of elements.\n",
    "dataset2 = SyntheticDataset2(num_samples=1000, input_dim=10, output_dim=1)\n",
    "\n",
    "# batch_size is set here\n",
    "batch_size = 32\n",
    "# buffer_size is set here\n",
    "buffer_size = 1000\n",
    "\n",
    "# Combines consecutive elements of this dataset into batches.\n",
    "# Randomly shuffles the elements of this dataset.\n",
    "dataloader = dataset2.shuffle(buffer_size=buffer_size).batch(batch_size=batch_size)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    print(inputs, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ValeoML)",
   "language": "python",
   "name": "valeoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
