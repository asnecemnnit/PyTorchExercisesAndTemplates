{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalPerceptron(tf.keras.Model):\n",
    "    def __init__(self, input_size, output_size, task_type='binary_classification'):\n",
    "        super(UniversalPerceptron, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(output_size)\n",
    "\n",
    "        if task_type == 'binary_classification':\n",
    "            self.activation = tf.keras.activations.sigmoid\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        elif task_type == 'multi_class_classification':\n",
    "            self.activation = tf.keras.activations.softmax\n",
    "            self.loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        elif task_type == 'regression':\n",
    "            self.activation = tf.keras.activations.linear\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid task_type. Supported types are 'binary_classification', 'multi_class_classification', and 'regression'.\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random training data\n",
    "    np.random.seed(42)\n",
    "    input_size = 2\n",
    "    data_size = 100\n",
    "    inputs = np.random.rand(data_size, input_size).astype(np.float32)\n",
    "    labels_binary_cls = np.random.randint(2, size=(data_size, 1)).astype(np.float32)\n",
    "    labels_multi_cls = np.random.randint(3, size=(data_size)).astype(np.int32)\n",
    "    labels_regression = np.random.rand(data_size, 1).astype(np.float32)\n",
    "\n",
    "    # Create a UniversalPerceptron model for binary classification\n",
    "    binary_cls_model = UniversalPerceptron(input_size, 1, task_type='binary_classification')\n",
    "\n",
    "    # Define optimizer\n",
    "    binary_cls_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Training for binary classification\n",
    "    for epoch in range(100):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = binary_cls_model(inputs)\n",
    "            loss = binary_cls_model.loss_function(labels_binary_cls, outputs)\n",
    "        \n",
    "        gradients = tape.gradient(loss, binary_cls_model.trainable_variables)\n",
    "        binary_cls_optimizer.apply_gradients(zip(gradients, binary_cls_model.trainable_variables))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/100], Loss: {loss.numpy():.4f}')\n",
    "\n",
    "    # Testing the trained model for binary classification\n",
    "    test_inputs = np.random.rand(5, input_size).astype(np.float32)\n",
    "    binary_cls_predictions = (binary_cls_model(test_inputs) >= 0.5).numpy()\n",
    "    print(\"Binary Classification Predictions:\", binary_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for multi-class classification\n",
    "    multi_cls_model = UniversalPerceptron(input_size, 3, task_type='multi_class_classification')\n",
    "\n",
    "    # Define optimizer\n",
    "    multi_cls_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Training for multi-class classification\n",
    "    for epoch in range(100):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = multi_cls_model(inputs)\n",
    "            loss = multi_cls_model.loss_function(labels_multi_cls, outputs)\n",
    "\n",
    "        gradients = tape.gradient(loss, multi_cls_model.trainable_variables)\n",
    "        multi_cls_optimizer.apply_gradients(zip(gradients, multi_cls_model.trainable_variables))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/100], Loss: {loss.numpy():.4f}')\n",
    "\n",
    "    # Testing the trained model for multi-class classification\n",
    "    multi_cls_predictions = np.argmax(multi_cls_model(test_inputs), axis=1)\n",
    "    print(\"Multi-Class Classification Predictions:\", multi_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for regression\n",
    "    regression_model = UniversalPerceptron(input_size, 1, task_type='regression')\n",
    "\n",
    "    # Define optimizer\n",
    "    regression_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Training for regression\n",
    "    for epoch in range(100):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = regression_model(inputs)\n",
    "            loss = regression_model.loss_function(labels_regression, outputs)\n",
    "\n",
    "        gradients = tape.gradient(loss, regression_model.trainable_variables)\n",
    "        regression_optimizer.apply_gradients(zip(gradients, regression_model.trainable_variables))\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/100], Loss: {loss.numpy():.4f}')\n",
    "\n",
    "    # Testing the trained model for regression\n",
    "    regression_predictions = regression_model(test_inputs).numpy()\n",
    "    print(\"Regression Predictions:\", regression_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ValeoML)",
   "language": "python",
   "name": "valeoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
