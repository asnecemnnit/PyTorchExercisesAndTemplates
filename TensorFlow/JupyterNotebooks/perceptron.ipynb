{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Activations:<ins>\n",
    "\n",
    "### <ins>Sigmoid:<ins>\n",
    "$ \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $\n",
    "\n",
    "### <ins>Softmax:<ins>\n",
    "$ \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $\n",
    "\n",
    "# <ins>Loss Functions:<ins>\n",
    "\n",
    "### <ins>Binary Cross-Entropy (BCE) Loss:<ins>\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right] $\n",
    "\n",
    "### <ins>Cross-Entropy Loss:<ins>\n",
    "Softmax function is often used to convert the raw model outputs (logits) into probabilities. Cross-Entropy Loss measures the dissimilarity between this predicted probability distribution and the true distribution (one-hot encoded labels).\n",
    "\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "          l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
    "          \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\} $\n",
    "\n",
    "### <ins>Mean-Squared Error (MSE) Loss:<ins>\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = \\left( x_n - y_n \\right)^2 $\n",
    "\n",
    "# <ins>Definitions:<ins>\n",
    "\n",
    "### <ins>Logits:<ins>\n",
    "$ \\text{logit}_i = \\log\\left(\\frac{p_i}{1 - \\sum_{j=1}^{K-1} p_j}\\right) $,\n",
    "all classes except the $i$-th class in denominator\n",
    "\n",
    "### <ins>Sigmoid Probability:</ins>\n",
    "$ p = \\frac{1}{1 + \\exp(-\\text{logit})} $\n",
    "\n",
    "\n",
    "### <ins>Softmax Probability:<ins>\n",
    "$ p_i = \\frac{\\exp(\\text{logit}_i)}{\\sum_{j=1}^{K} \\exp(\\text{logit}_j)} $,\n",
    "all classes including the $i$-th class in denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalPerceptron(tf.keras.Model):\n",
    "    def __init__(self, input_size, output_size, task_type=\"binary_classification\"):\n",
    "        super(UniversalPerceptron, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(output_size)\n",
    "\n",
    "        if task_type == \"binary_classification\":\n",
    "            self.activation = tf.keras.activations.sigmoid\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        elif task_type == \"multi_class_classification\":\n",
    "            self.activation = tf.keras.activations.softmax\n",
    "            self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        elif task_type == \"regression\":\n",
    "            self.activation = tf.keras.activations.linear\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid task_type. Supported types are 'binary_classification', 'multi_class_classification', and 'regression'.\"\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class UniversalPerceptronTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, inputs, labels, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            # tf.GradientTape context is used to record operations for automatic differentiation.\n",
    "            # It allows TensorFlow to compute gradients with respect to the variables inside the block.\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.model.loss_function(labels, outputs)\n",
    "            \n",
    "            # tape.gradient function computes the gradients of the loss with respect to the \n",
    "            # trainable variables (model parameters) using operations recorded in context of \n",
    "            # this tape. These gradients will be used to update the model during optimization.\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "            # optimizer's apply_gradients method updates the model's parameters using the \n",
    "            # computed gradients. The zip function combines each gradient with its corresponding \n",
    "            # trainable variable, forming pairs that are applied together to update the model.\n",
    "            self.optimizer.apply_gradients(\n",
    "                zip(gradients, self.model.trainable_variables)\n",
    "            )\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # tf.GradientTape() context manager is not used, TensorFlow won't compute gradients for \n",
    "        # following operations\n",
    "        predictions = self.model(inputs)\n",
    "        if isinstance(self.model.loss_function, tf.keras.losses.BinaryCrossentropy):\n",
    "            # class with sigmoid probability >= 0.5\n",
    "            return (predictions >= 0.5).numpy()\n",
    "        elif isinstance(\n",
    "            self.model.loss_function, tf.keras.losses.CategoricalCrossentropy\n",
    "        ):\n",
    "            # class with highest softmax probability\n",
    "            return np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            return predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random training data\n",
    "    np.random.seed(42)\n",
    "    input_size = 2\n",
    "    data_size = 100\n",
    "    inputs = np.random.rand(data_size, input_size).astype(np.float32)\n",
    "    labels_binary_cls = np.random.randint(2, size=(data_size, 1)).astype(np.float32)\n",
    "    labels_multi_cls = to_categorical(\n",
    "        np.random.randint(3, size=(data_size)).astype(np.int32), num_classes=3\n",
    "    )\n",
    "    labels_regression = np.random.rand(data_size, 1).astype(np.float32)\n",
    "\n",
    "    # Create a UniversalPerceptron model for binary classification\n",
    "    binary_cls_model = UniversalPerceptron(\n",
    "        input_size, 1, task_type=\"binary_classification\"\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    binary_cls_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for binary classification\n",
    "    binary_cls_trainer = UniversalPerceptronTrainer(\n",
    "        binary_cls_model, binary_cls_optimizer\n",
    "    )\n",
    "\n",
    "    # Train the model for binary classification\n",
    "    binary_cls_trainer.train(inputs, labels_binary_cls, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for binary classification\n",
    "    test_inputs = np.random.rand(5, input_size).astype(np.float32)\n",
    "    binary_cls_predictions = binary_cls_trainer.predict(test_inputs)\n",
    "    print(\"Binary Classification Predictions:\", binary_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for multi-class classification\n",
    "    multi_cls_model = UniversalPerceptron(\n",
    "        input_size, 3, task_type=\"multi_class_classification\"\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    multi_cls_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for multi-class classification\n",
    "    multi_cls_trainer = UniversalPerceptronTrainer(multi_cls_model, multi_cls_optimizer)\n",
    "\n",
    "    # Train the model for multi-class classification\n",
    "    multi_cls_trainer.train(inputs, labels_multi_cls, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for multi-class classification\n",
    "    multi_cls_predictions = multi_cls_trainer.predict(test_inputs)\n",
    "    print(\"Multi-Class Classification Predictions:\", multi_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for regression\n",
    "    regression_model = UniversalPerceptron(input_size, 1, task_type=\"regression\")\n",
    "\n",
    "    # Define optimizer\n",
    "    regression_optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for regression\n",
    "    regression_trainer = UniversalPerceptronTrainer(\n",
    "        regression_model, regression_optimizer\n",
    "    )\n",
    "\n",
    "    # Train the model for regression\n",
    "    regression_trainer.train(inputs, labels_regression, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for regression\n",
    "    regression_predictions = regression_trainer.predict(test_inputs)\n",
    "    print(\"Regression Predictions:\", regression_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ValeoML)",
   "language": "python",
   "name": "valeoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
