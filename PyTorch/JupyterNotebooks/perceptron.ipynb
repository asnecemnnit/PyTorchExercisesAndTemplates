{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>Activations:<ins>\n",
    "\n",
    "### <ins>Sigmoid:<ins>\n",
    "$ \\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)} $\n",
    "\n",
    "### <ins>Softmax:<ins>\n",
    "$ \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $\n",
    "\n",
    "# <ins>Loss Functions:<ins>\n",
    "\n",
    "### <ins>Binary Cross-Entropy (BCE) Loss:<ins>\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right] $\n",
    "\n",
    "### <ins>Cross-Entropy Loss:<ins>\n",
    "Softmax function is often used to convert the raw model outputs (logits) into probabilities. Cross-Entropy Loss measures the dissimilarity between this predicted probability distribution and the true distribution (one-hot encoded labels).\n",
    "\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "          l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
    "          \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\} $\n",
    "\n",
    "#### <ins>Example (Cross-Entropy Loss):<ins>\n",
    "\n",
    "Let the true labels $y$ be one-hot encoded as follows:\n",
    "\n",
    "$ y = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\quad \\text{(one-hot encoded labels)} $\n",
    "\n",
    "Assume the model predictions $x$ are as follows:\n",
    "\n",
    "$ x = \\begin{bmatrix} 1.5 & -0.8 & 0.3 \\\\ -0.4 & 2.1 & -1.0 \\end{bmatrix} \\quad \\text{(logits)} $\n",
    "\n",
    "Now, the cross-entropy loss for each sample is given by:\n",
    "\n",
    "$ L = \\begin{bmatrix} l_1 \\\\ l_2 \\end{bmatrix} $\n",
    "\n",
    "$ l_1 = -\\sum_{c=1}^C y_{1,c} \\log\\left(\\frac{\\exp(x_{1,c})}{\\sum_{k=1}^C \\exp(x_{1,k})}\\right) \\approx -y_{1,1} \\log\\left(\\frac{\\exp(1.5)}{\\exp(1.5) + \\exp(-0.8) + \\exp(0.3)}\\right) \\approx 0.48 $\n",
    "\n",
    "$ l_2 = -\\sum_{c=1}^C y_{2,c} \\log\\left(\\frac{\\exp(x_{2,c})}{\\sum_{k=1}^C \\exp(x_{2,k})}\\right) \\approx -y_{2,2} \\log\\left(\\frac{\\exp(2.1)}{\\exp(-0.4) + \\exp(2.1) + \\exp(-1.0)}\\right) \\approx 0.15 $\n",
    "\n",
    "So, $ L = \\begin{bmatrix} 0.48 \\\\ 0.15 \\end{bmatrix} $ would be the vector of losses for this example.\n",
    "\n",
    "### <ins>Mean-Squared Error (MSE) Loss:<ins>\n",
    "$ \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "        l_n = \\left( x_n - y_n \\right)^2 $\n",
    "\n",
    "# <ins>Definitions:<ins>\n",
    "\n",
    "### <ins>Logits:<ins>\n",
    "$ \\text{logit}_i = \\log\\left(\\frac{p_i}{1 - \\sum_{j=1}^{K-1} p_j}\\right) $,\n",
    "all classes except the $i$-th class in denominator\n",
    "\n",
    "### <ins>Sigmoid Probability:</ins>\n",
    "$ p = \\frac{1}{1 + \\exp(-\\text{logit})} $\n",
    "\n",
    "\n",
    "### <ins>Softmax Probability:<ins>\n",
    "$ p_i = \\frac{\\exp(\\text{logit}_i)}{\\sum_{j=1}^{K} \\exp(\\text{logit}_j)} $,\n",
    "all classes including the $i$-th class in denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, output_size, task_type=\"binary_classification\"):\n",
    "        super(UniversalPerceptron, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "        if task_type == \"binary_classification\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "            self.loss_function = nn.BCELoss()\n",
    "        elif task_type == \"multi_class_classification\":\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "            self.loss_function = nn.CrossEntropyLoss()\n",
    "        elif task_type == \"regression\":\n",
    "            self.activation = nn.Identity()\n",
    "            self.loss_function = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid task_type. Supported types are 'binary_classification', 'multi_class_classification', and 'regression'.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UniversalPerceptronTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, inputs, labels, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            # zero out the gradients of all trainable parameters before computing the gradients\n",
    "            # for the current batch. backward() method accumulates gradients by default, so \n",
    "            # calling zero_grad() ensures that the gradients are reset for each iteration\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.model.loss_function(outputs, labels)\n",
    "\n",
    "            # computes the gradients of the loss with respect to the parameters (weights and \n",
    "            # biases) using backpropagation. The gradients are stored in the grad attribute of \n",
    "            # each parameter\n",
    "            loss.backward()\n",
    "\n",
    "            # update the model's parameters based on the computed gradients\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # context manager that temporarily disables gradient computation. When inside this block, \n",
    "        # PyTorch does not track operations for gradient computation\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(inputs)\n",
    "            if isinstance(self.model.loss_function, nn.BCELoss):\n",
    "                # class with sigmoid probability >= 0.5\n",
    "                predictions = (outputs >= 0.5).float()\n",
    "            elif isinstance(self.model.loss_function, nn.CrossEntropyLoss):\n",
    "                # class with highest softmax probability\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "            else:\n",
    "                predictions = outputs\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate some random training data\n",
    "    np.random.seed(42)\n",
    "    input_size = 2\n",
    "    data_size = 100\n",
    "    inputs = torch.tensor(np.random.rand(data_size, input_size), dtype=torch.float32)\n",
    "    labels_binary_cls = torch.tensor(\n",
    "        np.random.randint(2, size=(data_size, 1)), dtype=torch.float32\n",
    "    )\n",
    "    labels_multi_cls = torch.tensor(\n",
    "        np.random.randint(3, size=(data_size)), dtype=torch.long\n",
    "    )\n",
    "    # In Pytorch, you can directly also use integer-encoded labels with nn.CrossEntropyLoss\n",
    "    labels_multi_cls = F.one_hot(\n",
    "        torch.tensor(np.random.randint(3, size=(data_size)), dtype=torch.long),\n",
    "        num_classes=3,\n",
    "    ).float()\n",
    "    labels_regression = torch.tensor(np.random.rand(data_size, 1), dtype=torch.float32)\n",
    "\n",
    "    # Create a UniversalPerceptron model for binary classification\n",
    "    binary_cls_model = UniversalPerceptron(\n",
    "        input_size, 1, task_type=\"binary_classification\"\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    binary_cls_optimizer = optim.SGD(binary_cls_model.parameters(), lr=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for binary classification\n",
    "    binary_cls_trainer = UniversalPerceptronTrainer(\n",
    "        binary_cls_model, binary_cls_optimizer\n",
    "    )\n",
    "\n",
    "    # Train the model for binary classification\n",
    "    binary_cls_trainer.train(inputs, labels_binary_cls, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for binary classification\n",
    "    test_inputs = torch.tensor(np.random.rand(5, input_size), dtype=torch.float32)\n",
    "    binary_cls_predictions = binary_cls_trainer.predict(test_inputs)\n",
    "    print(\"Binary Classification Predictions:\", binary_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for multi-class classification\n",
    "    multi_cls_model = UniversalPerceptron(\n",
    "        input_size, 3, task_type=\"multi_class_classification\"\n",
    "    )\n",
    "\n",
    "    # Define optimizer\n",
    "    multi_cls_optimizer = optim.SGD(multi_cls_model.parameters(), lr=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for multi-class classification\n",
    "    multi_cls_trainer = UniversalPerceptronTrainer(multi_cls_model, multi_cls_optimizer)\n",
    "\n",
    "    # Train the model for multi-class classification\n",
    "    multi_cls_trainer.train(inputs, labels_multi_cls, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for multi-class classification\n",
    "    multi_cls_predictions = multi_cls_trainer.predict(test_inputs)\n",
    "    print(\"Multi-Class Classification Predictions:\", multi_cls_predictions)\n",
    "\n",
    "    # Create a UniversalPerceptron model for regression\n",
    "    regression_model = UniversalPerceptron(input_size, 1, task_type=\"regression\")\n",
    "\n",
    "    # Define optimizer\n",
    "    regression_optimizer = optim.SGD(regression_model.parameters(), lr=0.01)\n",
    "\n",
    "    # Create a UniversalPerceptronTrainer instance for regression\n",
    "    regression_trainer = UniversalPerceptronTrainer(\n",
    "        regression_model, regression_optimizer\n",
    "    )\n",
    "\n",
    "    # Train the model for regression\n",
    "    regression_trainer.train(inputs, labels_regression, epochs=100)\n",
    "\n",
    "    # Test the trained model with new data for regression\n",
    "    regression_predictions = regression_trainer.predict(test_inputs)\n",
    "    print(\"Regression Predictions:\", regression_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ValeoML)",
   "language": "python",
   "name": "valeoml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
